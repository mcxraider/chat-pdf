{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Structured and multimodal RAG\n",
    "\n",
    "- We will use Unstructured to parse both text and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
    "- We will use LCEL to implement the chains used.\n",
    "\n",
    "Notebook for reference: https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "index = pc.Index('hsi-notes')\n",
    "namespace = 'Chapter-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ADOBE API to extract components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "\n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_index_llama(table_str):\n",
    "    class Header(BaseModel):\n",
    "        index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "        \n",
    "    parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    \n",
    "    ## Account for if unsure. -1 maybe?\n",
    "    template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "            You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "            Follow the format instructions carefully.\n",
    "            Table:\n",
    "            {table}\n",
    "            \n",
    "            {format_instructions}\n",
    "            '''\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"table\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | chat | parser\n",
    "    return chain.invoke({\"table\": table_str})\n",
    "\n",
    "def clean_values(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.replace('_x000D_', '').strip()\n",
    "    return x\n",
    "\n",
    "def get_table_check_string(df):\n",
    "    table_str = \"\"\n",
    "    for i in range(2):\n",
    "        if i ==1:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "        else:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "    return table_str\n",
    "\n",
    "def convert_table_to_str(df):\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \"\"\n",
    "        for col in df.columns:\n",
    "            sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "            row_sentence = \"\"\n",
    "            for i in range(len(sentences)):\n",
    "                row_sentence += sentences[i] + \"\\n\"\n",
    "            row_str += f\"{col}: {row_sentence}, \"\n",
    "        formatted = row_str[:-2]\n",
    "    return formatted\n",
    "    \n",
    "def get_table_meta(elements):\n",
    "    table_file_pages = {}\n",
    "    for el in elements:\n",
    "        # Using get to avoid KeyError and ensure 'filePaths' is not empty\n",
    "        file_paths = el.get('filePaths')\n",
    "        if file_paths:\n",
    "            page = el.get('Page', 'Unknown')  # Provide a default page number if missing\n",
    "            table_file_pages[file_paths[0]] = {\"Page\": page}\n",
    "    return table_file_pages\n",
    "\n",
    "def extract_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "    \n",
    "def get_table_pages_and_text_chunks(json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    table_file_pages = {}\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    \n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        raise\n",
    "\n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    # Process the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'Page': start_page + 1, 'Text': chunk})\n",
    " \n",
    "    # Obtaining the table metadata\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            file_paths = json_data['elements'][i].get('filePaths')\n",
    "            if file_paths:\n",
    "                page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                match = re.search(r'\\d+', file_paths[0])\n",
    "                table_index = match.group(0)\n",
    "                table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "    return table_file_pages, all_chunks\n",
    "\n",
    "def get_tables(extractor):\n",
    "    #The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for fname, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        df = df.applymap(clean_values)\n",
    "        df_str = get_table_check_string(df) \n",
    "        #dic = eval_table_index_llama(df_str)\n",
    "        #header_index = dic['index']\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "            \n",
    "        table_str = convert_table_to_str(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "    return table_dataframes\n",
    "\n",
    "def generate_tables(table_dataframes, table_file_pages):\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index, table_str in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    \n",
    "    return table_dfs, meta_table_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_embed(payload: str) -> str:\n",
    "        response = requests.post(dense_embedder_api, headers={\"Authorization\": f\"Bearer {hf_key}\"}, json=payload)\n",
    "        return response.json()\n",
    "\n",
    "class BM25Singleton:\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls, texts=None):\n",
    "        if cls._instance is None:\n",
    "            if texts is None:\n",
    "                raise ValueError(\"Initial texts required for the first initialization!\")\n",
    "            cls._instance = cls(texts)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        self.bm25 = BM25Encoder()\n",
    "        self.bm25.fit(texts)\n",
    "\n",
    "    def encode(self, queries):\n",
    "        return self.bm25.encode_documents(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pinecone_data(text_documents, table_dfs):\n",
    "    text_df = pd.DataFrame(text_documents)\n",
    "    tables_df = pd.DataFrame(table_dfs)\n",
    "    all_texts = text_df['Text'].tolist() + tables_df['Table'].tolist()\n",
    "    return all_texts, text_df, tables_df\n",
    "\n",
    "# THIS DOES NOT INCLUDE THE ACTUAL UPSERT OF THE DATA ONTO PINECONE YET\n",
    "def get_pinecone_data(text_df, tables_df, meta_table_batch, bm25):\n",
    "        table_df_dict = tables_df.to_dict(orient=\"records\") \n",
    "\n",
    "        # Section to embed and get out embeddings for the tables\n",
    "        table_chunks = tables_df['Table'].tolist()\n",
    "        table_sparse_embeddings = bm25.encode([combined for combined in meta_table_batch])\n",
    "        table_dense_embeddings = dense_embed(table_chunks)\n",
    "        \n",
    "        if not isinstance(table_dense_embeddings, list):\n",
    "            print(\"Embedding model not working properly\")\n",
    "            return None\n",
    "        \n",
    "            # Generate a list of IDs for the current batch\n",
    "        table_ids = ['vec' +str(x) for x in range(len(meta_table_batch))]\n",
    "        pinecone_table_upserts = []\n",
    "            \n",
    "        for _id, sparse, dense, meta in zip(table_ids, table_sparse_embeddings, table_dense_embeddings, table_df_dict):\n",
    "                pinecone_table_upserts.append({\n",
    "                    'id': _id,\n",
    "                    'values': dense,\n",
    "                    'sparse_values': sparse,\n",
    "                    'metadata': meta\n",
    "                })\n",
    "        \n",
    "        \n",
    "        # Section to embed and get out embeddings for the Texts\n",
    "        batched_pinecone_texts = []\n",
    "        batch_size = 32\n",
    "        for i in trange(0, len(text_df), batch_size):\n",
    "            i_end = min(i+batch_size, len(text_df)) # Determine the end index of the current batch\n",
    "            df_batch = text_df.iloc[i:i_end] # Extract the current batch from the DataFrame\n",
    "            text_df_batch_dict = df_batch.to_dict(orient=\"records\")\n",
    "\n",
    "            meta_text_batch = [\n",
    "                    f\"ElementType 'Text', Page {row['Page']}: {row['Text']}\" for _, row in text_df.iterrows()\n",
    "                ]\n",
    "            text_chunks = df_batch['Text'].tolist()\n",
    "            text_sparse_embeddings = bm25.encode([combined for combined in meta_text_batch])\n",
    "            text_dense_embeddings = dense_embed(text_chunks)\n",
    "            if not isinstance(text_dense_embeddings, list):\n",
    "                    print(\"Embedding model not working properly\")\n",
    "                    return None\n",
    "                \n",
    "            # Generate a list of IDs for the current batch\n",
    "            text_ids = ['vec' +str(x) for x in range(i + len(table_ids), i_end)]\n",
    "            pinecone_text_upserts = []\n",
    "                \n",
    "            for _id, sparse, dense, meta in zip(text_ids, text_sparse_embeddings, text_dense_embeddings, text_df_batch_dict):\n",
    "                    pinecone_text_upserts.append({\n",
    "                        'id': _id,\n",
    "                        'values': dense,\n",
    "                        'sparse_values': sparse,\n",
    "                        'metadata': meta\n",
    "                    })\n",
    "            batched_pinecone_texts.append(pinecone_text_upserts)\n",
    "        return batched_pinecone_texts, pinecone_table_upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upsert the pinecone texts and tables\n",
    "def upsert_pinecone_data(pinecone_text, pinecone_tables):  \n",
    "    def total_vector_count(pinecone_text, pinecone_tables):\n",
    "        total = len(pinecone_tables)\n",
    "        for batch in pinecone_text:\n",
    "            total += len(batch)\n",
    "        return total \n",
    "    \n",
    "    def check_upsert_success(index, namespace):\n",
    "        index_status = index.describe_index_stats()\n",
    "        return index_status['namespaces'][namespace]['vector_count'] == total_vector_count(pinecone_text, pinecone_tables)\n",
    "        \n",
    "            \n",
    "    # Upserting tables\n",
    "    index_stats = pc.describe_index(os.environ['PINECONE_INDEX_NAME'])\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        index.upsert(vectors = pinecone_tables, namespace=namespace)\n",
    "        tables_upserted = True\n",
    "    else:\n",
    "        print(\"Pinecone database not ready. Check set up or connection... \\n\")\n",
    "        return\n",
    "    if tables_upserted:\n",
    "        for batch_num in trange(len(pinecone_text)):\n",
    "            if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "                index.upsert(vectors = pinecone_text[batch_num], namespace=namespace)\n",
    "            else:\n",
    "                print(\"Pinecone database not ready. Check set up or connection... \\n\")\n",
    "                return\n",
    "            print(f\"Batch {batch_num + 1} upserted\")\n",
    "            time.sleep(2)\n",
    "        if check_upsert_success(index, namespace):\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Not all vectors were upserted. Exiting...\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Something went wrong while upserting the tables to the pinecone index. \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_table_matches(result):\n",
    "        print(f\"Namespace searched: {result['namespace']}\\n\")\n",
    "        num_results = len(result['matches'])\n",
    "        print(f\"Top {num_results} relevant chunks found:\\n\")\n",
    "        for i in range(num_results):\n",
    "            print(f\"Found on page {int(result['matches'][i]['metadata']['Page'])}:\")\n",
    "            print(f\"{result['matches'][i]['metadata']['Table']}\")\n",
    "            print(f\"Dotproduct score: {result['matches'][i]['score']}\")\n",
    "            print(\"-\" * 80)\n",
    "                   \n",
    "def get_llm_context_table(query, top_k, model):\n",
    "    index_stats = pc.describe_index(os.environ['PINECONE_INDEX_NAME'])\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        dense_query = dense_embed(query)\n",
    "        sparse_query = model.encode_queries(query)\n",
    "        relevant_matches = index.query( \n",
    "            namespace=namespace,\n",
    "            filter={ \n",
    "                    'ElementType': 'Table'\n",
    "                },\n",
    "            top_k=top_k, \n",
    "            vector=dense_query, \n",
    "            sparse_vector=sparse_query, \n",
    "            include_metadata=True\n",
    "            )\n",
    "    pretty_print_table_matches(relevant_matches)\n",
    "    # ideally its just to combine the first 2 matches. Or maybe to go by dotproduct score and difference \n",
    "    context = \"\"\n",
    "    for i in range(len(relevant_matches['matches'])):\n",
    "        context += f\"Page: {int(relevant_matches['matches'][i]['metadata']['Page'])} \" + relevant_matches['matches'][i]['metadata']['Table'] + \"\\n\"\n",
    "    return context\n",
    "\n",
    "def pretty_print_text_matches(result):\n",
    "        print(f\"Namespace searched: {result['namespace']}\\n\")\n",
    "        num_results = len(result['matches'])\n",
    "        print(f\"Top {num_results} relevant chunks found:\\n\")\n",
    "        for i in range(num_results):\n",
    "            print(f\"Found on page {int(result['matches'][i]['metadata']['Page'])}:\")\n",
    "            print(f\"{result['matches'][i]['metadata']['Text']}\")\n",
    "            print(f\"Dotproduct score: {result['matches'][i]['score']}\")\n",
    "            print(\"-\" * 80) \n",
    "            \n",
    "def get_llm_context_text(query, top_k):\n",
    "    index_stats = pc.describe_index(os.environ['PINECONE_INDEX_NAME'])\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        dense_query = dense_embed(query)\n",
    "        bm25_instance = BM25Singleton.get_instance()\n",
    "        sparse_query = bm25_instance.encode(query)\n",
    "        relevant_matches = index.query( \n",
    "            namespace=namespace,\n",
    "            top_k=top_k, \n",
    "            vector=dense_query, \n",
    "            sparse_vector=sparse_query, \n",
    "            include_metadata=True\n",
    "            )\n",
    "    pretty_print_text_matches(relevant_matches)\n",
    "    # ideally its just to combine the first 2 matches. Or maybe to go by dotproduct score and difference \n",
    "    context = \"\"\n",
    "    for i in range(len(relevant_matches['matches'])):\n",
    "        context += f\"Page: {int(relevant_matches['matches'][i]['metadata']['Page'])} \" + relevant_matches['matches'][i]['metadata']['Text'] + \"\\n\"\n",
    "    return context\n",
    "\n",
    "def llama_chat(user_question, k):\n",
    "    context = get_llm_context_text(user_question, k)\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    system = '''\n",
    "You are a Science Professor in a university. \n",
    "Given the user's question and relevant excerpts from a set of school notes about scientific methodology and the history of science,\n",
    "you will also answer the question in a professional tone by including direct quotes from the notes, \\\n",
    "along with the page number where the answer or answers can be found.\n",
    "\n",
    "For example:\n",
    "Answer:\n",
    "Reference Page(s): \n",
    "'''\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human)\n",
    "        ])\n",
    "    chain = prompt | chat\n",
    "    return chain.invoke({\"text\": f\"User Question: \" + user_question + \"\\n\\nRelevant section in textbook:\\n\\n\" + context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pinecone index to test the retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:09:36,186 - INFO - Started uploading asset\n",
      "2024-06-17 14:09:39,535 - INFO - Finished uploading asset\n",
      "2024-06-17 14:09:39,539 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-06-17 14:09:40,832 - INFO - Started getting job result\n",
      "2024-06-17 14:09:51,580 - INFO - Finished polling for status\n",
      "2024-06-17 14:09:51,583 - INFO - Finished getting job result\n",
      "2024-06-17 14:09:51,583 - INFO - Started getting content\n",
      "2024-06-17 14:09:51,944 - INFO - Finished getting content\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a65568dd50a45caafb9ffdf932a9ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:29<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/HSI1000-chapter1.pdf'\n",
    "extractor = ExtractTextTableInfoFromPDF(file_path)\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = extract_data(extracted_data)\n",
    "table_file_pages, text_chunks = get_table_pages_and_text_chunks(pdf_data)\n",
    "table_dataframes = get_tables(extractor)\n",
    "table_dfs, meta_table_batch = generate_tables(table_dataframes, table_file_pages)\n",
    "all_texts, text_df, tables_df= prepare_pinecone_data(text_chunks, table_dfs)\n",
    "bm25_instance = BM25Singleton.get_instance(texts=all_texts)\n",
    "pinecone_text, pinecone_tables = get_pinecone_data(text_df, tables_df, meta_table_batch, bm25_instance)\n",
    "# Only uncomment if you want to upsert a new set of data\n",
    "# upsert_pinecone_data(pinecone_text, pinecone_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace searched: Chapter-1\n",
      "\n",
      "Top 3 relevant chunks found:\n",
      "\n",
      "Found on page 10:\n",
      "Galileo was somehow able to ignore the reining Aristotelian worldview and look more closely at the behavior of objects slowing down and coming to rest. His keen eye and intellect were not contaminated by expectation or belief in the Aristotelian worldview\n",
      "Dotproduct score: 0.432265848\n",
      "--------------------------------------------------------------------------------\n",
      "Found on page 12:\n",
      ", (3) he knew all too well what was based on fact and conjecture in the Aristotelian view of how objects moved, and he certainly made sure his observations were (4) not contaminated by expectation or belief\n",
      "Dotproduct score: 0.431616575\n",
      "--------------------------------------------------------------------------------\n",
      "Found on page 17:\n",
      "It was truly shattering because, until this time, the motion of heavenly bodies and the motion of things on Earth were thought to be entirely unrelated. Indeed, the Aristotelian worldview required that they were entirely different because the planets were literally thought to be a part of heaven\n",
      "Dotproduct score: 0.418641657\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 09:41:26,247 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The Aristotelian worldview posits that the motion of heavenly bodies and the motion of things on Earth are entirely unrelated. According to this view, the planets are literally thought to be a part of heaven, and the motion of heavenly bodies is considered to be entirely different from the motion of objects on Earth.\n",
      "\n",
      "Reference Page(s): 10, 12, 17\n"
     ]
    }
   ],
   "source": [
    "answer = llama_chat(\"What was the Aristotelian worldview?\", 3)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROBLEM NOW: Case Sensitivity\n",
    "\n",
    "Problem: Some retrieval models and search engines are case-sensitive, meaning they treat \"Aristotelian\" and \"aristotelian\" as different terms.\n",
    "Solution: Ensure that both your queries and documents are normalized to the same case (usually lowercase) before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cohere re rank of the outputs. \n",
    "- https://medium.aiplanet.com/advanced-rag-cohere-re-ranker-99acc941601c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere re ranking\n",
    "Cohere Re-rank is a re-ranking technique used to enhance the retrieval process in Retrieval-Augmented Generation (RAG) systems. It focuses on refining the set of documents retrieved in response to a query to ensure that the most relevant and contextually appropriate documents are prioritized for generating the final response. Here’s a detailed explanation:\n",
    "\n",
    "Cohere Re-rank Method:\n",
    "### Cross-Encoder Architecture:\n",
    "The core of Cohere Re-rank lies in its use of a cross-encoder model. Unlike bi-encoder models that independently encode the query and documents and then compute similarities, a cross-encoder processes the query and documents jointly.\n",
    "Joint Processing: This means that the query and each candidate document are concatenated and fed into the model together. The cross-encoder can then consider the interactions between the query and the document more deeply, resulting in a more accurate relevance score.\n",
    "\n",
    "### Re-ranking Process:\n",
    "Initial Retrieval: The process begins with an initial retrieval phase, where a set of documents is fetched based on their relevance to the query using a simpler, faster retrieval method (e.g., BM25 or dense retrieval models).\n",
    "Re-ranking Phase: The retrieved documents are then re-evaluated using the cross-encoder. Each document-query pair is input into the cross-encoder, which outputs a relevance score for each pair.\n",
    "Prioritization: Documents are re-ranked based on these relevance scores. The documents with higher scores are deemed more relevant and are given higher priority in the final set of documents used for generating the response.\n",
    "\n",
    "### Benefits of Cohere Re-rank:\n",
    "Enhanced Relevance: By jointly analyzing the query and documents, the cross-encoder can capture subtle nuances and interactions that simpler methods might miss. This leads to more accurate relevance assessments.\n",
    "Contextual Understanding: The cross-encoder’s ability to consider the entire context of the query-document pair allows it to better understand and rank the documents based on their true relevance to the query.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "Computationally Intensive: Cross-encoders are more computationally expensive than bi-encoders because they require processing each query-document pair individually, rather than independently encoding the query and documents once.\n",
    "Latency: The increased computational load can lead to higher latency in the retrieval process, which might be a concern in real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature to pipe the xlsx tables to something the user can download?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore sentence window retrieval using llamaindex. Might need to change docsearch variable// retrieval variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
